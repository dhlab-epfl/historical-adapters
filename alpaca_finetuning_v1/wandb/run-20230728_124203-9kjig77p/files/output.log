
| distributed init (rank 0): env://, gpu 0
[12:42:06.783731] job dir: /home/sooh/historical-adapters/alpaca_finetuning_v1
[12:42:06.784304] Namespace(accum_iter=1,
adapter_layer=30,
adapter_len=10,
batch_size=4,
blr=0.009,
data_path='/instruction_dataset/',
device='cuda',
dist_backend='nccl',
dist_on_itp=False,
dist_url='env://',
distributed=True,
epochs=10,
gpu=0,
llama_model_path='/data1/data/sooh-data/llama/',
local_rank=8,
log_dir='./output_dir',
lr=None,
max_seq_len=512,
min_lr=0.0,
model='Llama7B_adapter',
num_workers=10,
output_dir='/data1/data/sooh-data/llama/hipe/checkpoint-prompt2/',
pin_mem=True,
rank=0,
resume='',
seed=0,
start_epoch=0,
warmup_epochs=2,
weight_decay=0.02,
world_size=4)
[12:42:07.078852] =================DATA VALIDATION=================
[12:42:07.079295] <__main__.InstructionDataset object at 0x7f805c3ea8e0>
[12:42:07.079449] <__main__.InstructionDataset object at 0x7f7fb0d65d60>
[12:42:07.079639] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f7fa41fc0a0>
[12:43:29.255529] [12:43:29.256023] [12:43:29.256142] [12:43:29.256235] [12:43:29.256329] [12:43:29.256417] [12:43:29.256503] [12:43:29.256599] [12:43:29.256702]
Traceback (most recent call last):
  File "finetuning_hipe_prompt2.py", line 398, in <module>
    main(args)
  File "finetuning_hipe_prompt2.py", line 316, in main
    model = models_llama_adapter.__dict__[args.model](args)
  File "/home/sooh/historical-adapters/alpaca_finetuning_v1/models_llama_adapter.py", line 18, in Llama7B_adapter
    checkpoint = torch.load(llama_model_path  + model_name + '/consolidated.00.pth', map_location="cpu")
  File "/home/sooh/.conda/envs/myenv/lib/python3.8/site-packages/torch/serialization.py", line 809, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/home/sooh/.conda/envs/myenv/lib/python3.8/site-packages/torch/serialization.py", line 1172, in _load
    result = unpickler.load()
  File "/home/sooh/.conda/envs/myenv/lib/python3.8/site-packages/torch/serialization.py", line 1142, in persistent_load
    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/home/sooh/.conda/envs/myenv/lib/python3.8/site-packages/torch/serialization.py", line 1112, in load_tensor
    storage = zip_file.get_storage_from_record(name, numel, torch.UntypedStorage)._typed_storage()._untyped_storage
KeyboardInterrupt