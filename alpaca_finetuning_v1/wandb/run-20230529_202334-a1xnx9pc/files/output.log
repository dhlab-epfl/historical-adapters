
| distributed init (rank 0): env://, gpu 0
[20:23:42.738229] job dir: /home/sooh/historical-adapters/alpaca_finetuning_v1
[20:23:42.739566] Namespace(accum_iter=1,
adapter_layer=30,
adapter_len=10,
batch_size=4,
blr=0.009,
data_path='/instruction_dataset/',
device='cuda',
dist_backend='nccl',
dist_on_itp=False,
dist_url='env://',
distributed=True,
epochs=30,
gpu=0,
llama_model_path='/data1/data/sooh-data/llama/',
local_rank=-1,
log_dir='./output_dir',
lr=None,
max_seq_len=512,
min_lr=0.0,
model='Llama7B_adapter',
num_workers=10,
output_dir='/data1/data/sooh-data/llama/archiv/checkpoint/',
pin_mem=True,
rank=0,
resume='',
seed=0,
start_epoch=0,
warmup_epochs=2,
weight_decay=0.02,
world_size=8)
[20:24:31.854492] =================DATA VALIDATION=================
[20:24:31.855056] <__main__.InstructionDataset object at 0x7fadcb2a6850>
[20:24:31.855192] <__main__.InstructionDataset object at 0x7fad1fc201f0>
[20:24:31.855387] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fad1f971ee0>
[20:24:37.078131] /data1/data/sooh-data/llama/7B/consolidated.00.pth
[20:24:39.377566] Model = Transformer(
  (tok_embeddings): Embedding(32000, 4096)
  (adapter_query): Embedding(300, 4096)
  (criterion): CrossEntropyLoss()
  (layers): ModuleList(
    (0-31): 32 x TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=4096, out_features=4096, bias=False)
        (wk): Linear(in_features=4096, out_features=4096, bias=False)
        (wv): Linear(in_features=4096, out_features=4096, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=11008, bias=False)
        (w2): Linear(in_features=11008, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=11008, bias=False)
      )
      (attention_norm): RMSNorm()
      (ffn_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=32000, bias=False)
)
[20:24:39.377964] base lr: 9.00e-03
[20:24:39.378092] actual lr: 1.12e-03
[20:24:39.378193] accumulate grad iterations: 1
[20:24:39.378289] effective batch size: 32
[20:24:41.827589] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001125
    maximize: False
    weight_decay: 0.0
Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001125
    maximize: False
    weight_decay: 0.02
)
[20:24:41.828024] Start training for 30 epochs
[20:24:41.829639] log_dir: ./output_dir
[20:24:44.250287] Epoch: [0]  [    0/13311]  eta: 8:56:47  lr: 0.000000  closs: 2.8477 (2.8477)  time: 2.4196  data: 0.9815  max mem: 36513
[20:24:48.937019] Epoch: [0]  [   10/13311]  eta: 2:23:11  lr: 0.000000  closs: 3.2422 (3.4341)  time: 0.6459  data: 0.0893  max mem: 36554
[20:24:53.623085] Epoch: [0]  [   20/13311]  eta: 2:04:22  lr: 0.000001  closs: 3.6055 (3.5303)  time: 0.4685  data: 0.0001  max mem: 36554
[20:24:58.312382] Epoch: [0]  [   30/13311]  eta: 1:57:39  lr: 0.000001  closs: 3.7305 (3.7589)  time: 0.4687  data: 0.0002  max mem: 36554
[20:25:02.997487] Epoch: [0]  [   40/13311]  eta: 1:54:09  lr: 0.000002  closs: 3.6191 (3.6587)  time: 0.4686  data: 0.0002  max mem: 36554
[20:25:04.107820] [20:25:04.108195] [20:25:04.108353] [20:25:04.108465] [20:25:04.108566] [20:25:04.108666] [20:25:04.108780] [20:25:04.108895] [20:25:04.109006] [20:25:04.109121] [20:25:04.109238] [20:25:04.109353] [20:25:04.109499]
Traceback (most recent call last):
  File "finetuning.py", line 351, in <module>
    main(args)
  File "finetuning.py", line 308, in main
    train_stats = train_one_epoch(
  File "/home/sooh/historical-adapters/alpaca_finetuning_v1/engine_finetuning.py", line 35, in train_one_epoch
    c_loss = model(examples, labels)
  File "/home/sooh/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/sooh/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/sooh/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
  File "/home/sooh/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/sooh/historical-adapters/alpaca_finetuning_v1/llama/model.py", line 251, in forward
    h = layer(h, start_pos, freqs_cis, mask, adapter[adapter_index].half())
  File "/home/sooh/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/sooh/historical-adapters/alpaca_finetuning_v1/llama/model.py", line 198, in forward
    h = x + self.attention.forward(self.attention_norm(x), start_pos, freqs_cis, mask, adapter)
  File "/home/sooh/historical-adapters/alpaca_finetuning_v1/llama/model.py", line 133, in forward
    extra_mask = torch.zeros(1, 1, seqlen, adapter_len).to(mask)
KeyboardInterrupt