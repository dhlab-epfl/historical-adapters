
| distributed init (rank 0): env://, gpu 0
[09:42:34.662708] job dir: /home/sooh/historical-adapters/alpaca_finetuning_v1
[09:42:34.663522] Namespace(accum_iter=1,
adapter_layer=30,
adapter_len=10,
batch_size=4,
blr=0.009,
data_path='/instruction_dataset/',
device='cuda',
dist_backend='nccl',
dist_on_itp=False,
dist_url='env://',
distributed=True,
epochs=10,
gpu=0,
llama_model_path='/data1/data/sooh-data/llama/',
local_rank=8,
log_dir='./output_dir',
lr=None,
max_seq_len=512,
min_lr=0.0,
model='Llama7B_adapter',
num_workers=10,
output_dir='/data1/data/sooh-data/llama/hipe/checkpoint-prompt1/',
pin_mem=True,
rank=0,
resume='',
seed=0,
start_epoch=0,
warmup_epochs=2,
weight_decay=0.02,
world_size=4)
[09:42:35.404645] (tensor([    1, 29871,    13,  4706,   887,   526,  1985,   408,   263,  4257,
         7855, 19679, 17924,   322,   596,  3414,   338,   304,  3858,   263,
         2183,  1426,   411,  4257,  7855, 11073, 29889,    13,  4706,  3575,
         3414,   338,   304, 12439,   322,  3858,   738,  4257, 16212,  2198,
          297,   278,  1426, 29889, 29871,    13,  4706,   450,  4257,  7855,
        11073,   393,   366,   674,   367,   773,   526,   323,  8890,   313,
         2230,   511, 11247, 29907,  8098,   313,  5479,   511,   349,  1001,
         3094,   313, 10532,   511,  6323, 29954,  2190, 26664,  8098,   313,
         6388,  2133,   511,   322, 13756, 14849,  1783,   313,  4704,   467,
           13,   308,    13,  4706,  6058, 29923, 29901,  3575,  1962,  3402,
          881,   367,   263,  4663,  3402, 29892,   988,  1269,   848, 11624,
          310,   263,  1734,   515,   278,  1881,  1426,   322,   967,  6590,
         4257,  7855,  3858, 29889,    13,    13,  4706,  2672, 12336, 29901,
        11698, 29965, 29963,  6670, 17101,   317,  3120, 29903,  1660, 29903,
          813,  1174, 29871, 29896, 29947, 29947, 29955,  1919,   425, 21903,
          480,  5582,   868,  1632,   329,   492,   269,   525,   707,  1035,
        29878,   434,   316, 29871, 29946, 29900, 13926,  2056,   301,   525,
        15477, 16731,  1044, 27751, 29871, 29896, 29906, 29892, 29900, 29900,
        29900, 18490,   869,  1174,  3024, 29871, 29941, 29900, 29892, 29900,
        29900, 29900,  1424,   869,  4625,  4370,  1224,   743,  1465,   425,
          274,  1759,   344,  9484,  1318,  1671,  1014,  7316,  1089,   966,
          867, 30000,  1960,   869,   382,  1054,   267,   316, 18165,  1056,
         2658,   869,    13,   308,    13,  4706, 19474, 12336, 29901,    13,
         4706,   518, 10998, 10041,  2396,   525, 15307,   742,   525,   726,
         2396,   525,  2369, 29871, 29896, 29947, 29947, 29955, 16675, 11117,
        10041,  2396,   525,  1955, 29954,  2190, 26664,  8098,   742,   525,
          726,  2396,   525,  6295,   455,  7342,   480,  5582,   868,  1632,
          329,   492, 16675, 11117, 10041,  2396,   525, 16652,  8098,   742,
          525,   726,  2396,   525, 29931,  1485, 11276, 16675, 11117, 10041,
         2396,   525, 16652,  8098,   742,   525,   726,  2396,   525, 15462,
        15792, 16675, 11117, 10041,  2396,   525, 16652,  8098,   742,   525,
          726,  2396,   525, 29933, 11795, 16675, 11117, 10041,  2396,   525,
        16652,  8098,   742,   525,   726,  2396,   525, 12432,   790,  4425,
        16675, 11117, 10041,  2396,   525, 16652,  8098,   742,   525,   726,
         2396,   525,  2928,   336, 16675, 11117, 10041,  2396,   525, 16652,
         8098,   742,   525,   726,  2396,   525,  6028, 20838,   601, 16675,
        11117, 10041,  2396,   525, 16652,  8098,   742,   525,   726,  2396,
          525, 29934,   608, 16675, 11117, 10041,  2396,   525, 16652,  8098,
          742,   525,   726,  2396,   525, 29925,   497,  8520, 16675, 11117,
        10041,  2396,   525, 16652,  8098,   742,   525,   726,  2396,   525,
         4620,  6973,  5411, 16675, 11117, 10041,  2396,   525, 16652,  8098,
          742,   525,   726,  2396,   525,  2816,   941,  8862,  5289, 16675,
        11117, 10041,  2396,   525, 15307,   742,   525,   726,  2396,   525,
          700, 29871, 29896, 29955,  7005, 29871, 29896, 29947, 29947, 29945,
        16675, 11117, 10041,  2396,   525, 13171,  3094,   742,   525,   726,
         2396,   525, 29933, 23067,  1919,  8774,  3068,   293,   819,   818,
          382,   305,   497,   575, 16675, 11117, 10041,  2396,   525, 15307,
          742,   525,   726,  2396,   525,   311, 29871, 29896, 29947, 29955,
        29947,   818, 29871, 29896, 29947, 29947, 29946, 16675, 11117, 10041,
         2396,   525, 15307,   742,   525,   726,  2396,   525, 29896, 29947,
        29955, 29947, 29892, 29906, 29892, 29953, 29900, 29947, 16675, 11117,
        10041,  2396]), tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,   518, 10998, 10041,  2396,   525, 15307,   742,   525,   726,
         2396,   525,  2369, 29871, 29896, 29947, 29947, 29955, 16675, 11117,
        10041,  2396,   525,  1955, 29954,  2190, 26664,  8098,   742,   525,
          726,  2396,   525,  6295,   455,  7342,   480,  5582,   868,  1632,
          329,   492, 16675, 11117, 10041,  2396,   525, 16652,  8098,   742,
          525,   726,  2396,   525, 29931,  1485, 11276, 16675, 11117, 10041,
         2396,   525, 16652,  8098,   742,   525,   726,  2396,   525, 15462,
        15792, 16675, 11117, 10041,  2396,   525, 16652,  8098,   742,   525,
          726,  2396,   525, 29933, 11795, 16675, 11117, 10041,  2396,   525,
        16652,  8098,   742,   525,   726,  2396,   525, 12432,   790,  4425,
        16675, 11117, 10041,  2396,   525, 16652,  8098,   742,   525,   726,
         2396,   525,  2928,   336, 16675, 11117, 10041,  2396,   525, 16652,
         8098,   742,   525,   726,  2396,   525,  6028, 20838,   601, 16675,
        11117, 10041,  2396,   525, 16652,  8098,   742,   525,   726,  2396,
          525, 29934,   608, 16675, 11117, 10041,  2396,   525, 16652,  8098,
          742,   525,   726,  2396,   525, 29925,   497,  8520, 16675, 11117,
        10041,  2396,   525, 16652,  8098,   742,   525,   726,  2396,   525,
         4620,  6973,  5411, 16675, 11117, 10041,  2396,   525, 16652,  8098,
          742,   525,   726,  2396,   525,  2816,   941,  8862,  5289, 16675,
        11117, 10041,  2396,   525, 15307,   742,   525,   726,  2396,   525,
          700, 29871, 29896, 29955,  7005, 29871, 29896, 29947, 29947, 29945,
        16675, 11117, 10041,  2396,   525, 13171,  3094,   742,   525,   726,
         2396,   525, 29933, 23067,  1919,  8774,  3068,   293,   819,   818,
          382,   305,   497,   575, 16675, 11117, 10041,  2396,   525, 15307,
          742,   525,   726,  2396,   525,   311, 29871, 29896, 29947, 29955,
        29947,   818, 29871, 29896, 29947, 29947, 29946, 16675, 11117, 10041,
         2396,   525, 15307,   742,   525,   726,  2396,   525, 29896, 29947,
        29955, 29947, 29892, 29906, 29892, 29953, 29900, 29947, 16675, 11117,
        10041,  2396]), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1.]))
[09:42:35.413520] =================DATA VALIDATION=================
[09:42:35.413772] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f02f521a130>
Traceback (most recent call last):
  File "finetuning_hipe_prompt1.py", line 411, in <module>
    main(args)
  File "finetuning_hipe_prompt1.py", line 329, in main
    model = models_llama_adapter.__dict__[args.model](args)
  File "/home/sooh/historical-adapters/alpaca_finetuning_v1/models_llama_adapter.py", line 18, in Llama7B_adapter
    checkpoint = torch.load(llama_model_path  + model_name + '/consolidated.00.pth', map_location="cpu")
  File "/home/sooh/.conda/envs/myenv/lib/python3.8/site-packages/torch/serialization.py", line 809, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/home/sooh/.conda/envs/myenv/lib/python3.8/site-packages/torch/serialization.py", line 1172, in _load
    result = unpickler.load()
  File "/home/sooh/.conda/envs/myenv/lib/python3.8/site-packages/torch/serialization.py", line 1142, in persistent_load
    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/home/sooh/.conda/envs/myenv/lib/python3.8/site-packages/torch/serialization.py", line 1112, in load_tensor
    storage = zip_file.get_storage_from_record(name, numel, torch.UntypedStorage)._typed_storage()._untyped_storage
KeyboardInterrupt
[09:43:26.650203] [09:43:26.650651] [09:43:26.650782] [09:43:26.650886] [09:43:26.651006] [09:43:26.651137] [09:43:26.651274] [09:43:26.651412] [09:43:26.651542]