
| distributed init (rank 0): env://, gpu 0
[15:50:39.408276] job dir: /home/sooh/historical-adapters/alpaca_finetuning_v1
[15:50:39.409013] Namespace(accum_iter=1,
adapter_layer=30,
adapter_len=10,
batch_size=4,
blr=0.009,
data_path='/instruction_dataset/',
device='cuda',
dist_backend='nccl',
dist_on_itp=False,
dist_url='env://',
distributed=True,
epochs=30,
gpu=0,
llama_model_path='/data1/data/sooh-data/llama/',
local_rank=-1,
log_dir='./output_dir',
lr=None,
max_seq_len=512,
min_lr=0.0,
model='Llama7B_adapter',
num_workers=10,
output_dir='/data1/data/sooh-data/llama/checkpoint3/',
pin_mem=True,
rank=0,
resume='',
seed=0,
start_epoch=0,
warmup_epochs=2,
weight_decay=0.02,
world_size=4)
[15:50:39.432570] =================DATA VALIDATION=================
[15:50:39.432894] <__main__.InstructionDataset object at 0x7f380600a8b0>
[15:50:39.432998] <__main__.InstructionDataset object at 0x7f374e0c6b20>
[15:50:39.433258] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f376599ca90>
[15:50:44.215458] /data1/data/sooh-data/llama/7B/consolidated.00.pth
[15:50:46.002757] Model = Transformer(
  (tok_embeddings): Embedding(32000, 4096)
  (adapter_query): Embedding(300, 4096)
  (criterion): CrossEntropyLoss()
  (layers): ModuleList(
    (0-31): 32 x TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=4096, out_features=4096, bias=False)
        (wk): Linear(in_features=4096, out_features=4096, bias=False)
        (wv): Linear(in_features=4096, out_features=4096, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=11008, bias=False)
        (w2): Linear(in_features=11008, out_features=4096, bias=False)
        (w3): Linear(in_features=4096, out_features=11008, bias=False)
      )
      (attention_norm): RMSNorm()
      (ffn_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=32000, bias=False)
)
[15:50:46.003034] base lr: 9.00e-03
[15:50:46.003153] actual lr: 5.62e-04
[15:50:46.003243] accumulate grad iterations: 1
[15:50:46.003331] effective batch size: 16
[15:50:46.402855] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005625
    maximize: False
    weight_decay: 0.0
Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005625
    maximize: False
    weight_decay: 0.02
)
[15:50:46.403227] Start training for 30 epochs
[15:50:46.404815] log_dir: ./output_dir
[15:50:48.530265] Epoch: [0]  [  0/795]  eta: 0:28:08  lr: 0.000000  closs: 2.5898 (2.5898)  time: 2.1243  data: 1.2518  max mem: 36518
[15:50:53.206161] Epoch: [0]  [ 10/795]  eta: 0:08:05  lr: 0.000004  closs: 2.5898 (2.6000)  time: 0.6181  data: 0.1139  max mem: 36548
[15:50:57.881150] Epoch: [0]  [ 20/795]  eta: 0:07:03  lr: 0.000007  closs: 2.6094 (2.6317)  time: 0.4674  data: 0.0001  max mem: 36548
[15:51:02.558832] Epoch: [0]  [ 30/795]  eta: 0:06:38  lr: 0.000011  closs: 2.5957 (2.6066)  time: 0.4675  data: 0.0001  max mem: 36548
[15:51:07.237813] Epoch: [0]  [ 40/795]  eta: 0:06:23  lr: 0.000014  closs: 2.5918 (2.6031)  time: 0.4677  data: 0.0001  max mem: 36548
[15:51:11.930430] Epoch: [0]  [ 50/795]  eta: 0:06:12  lr: 0.000018  closs: 2.5820 (2.5955)  time: 0.4685  data: 0.0001  max mem: 36548
[15:51:16.613174] Epoch: [0]  [ 60/795]  eta: 0:06:03  lr: 0.000021  closs: 2.5137 (2.5741)  time: 0.4687  data: 0.0001  max mem: 36548
[15:51:21.294310] Epoch: [0]  [ 70/795]  eta: 0:05:56  lr: 0.000025  closs: 2.5703 (2.5708)  time: 0.4681  data: 0.0002  max mem: 36548
[15:51:24.499018] [15:51:24.499385] [15:51:24.499539] [15:51:24.499678] [15:51:24.499780] [15:51:24.499894] [15:51:24.500033] [15:51:24.500192]
Traceback (most recent call last):
  File "finetuning.py", line 326, in <module>
    main(args)
  File "finetuning.py", line 283, in main
    train_stats = train_one_epoch(
  File "/home/sooh/historical-adapters/alpaca_finetuning_v1/engine_finetuning.py", line 46, in train_one_epoch
    loss_scaler(loss, optimizer, parameters=model.parameters(),
  File "/home/sooh/historical-adapters/alpaca_finetuning_v1/util/misc.py", line 258, in __call__
    self._scaler.scale(loss).backward(create_graph=create_graph)
  File "/home/sooh/.conda/envs/myenv/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/sooh/.conda/envs/myenv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt