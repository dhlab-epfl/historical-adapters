
| distributed init (rank 0): env://, gpu 0
[09:14:18.332599] job dir: /home/sooh/historical-adapters/alpaca_finetuning_v1
[09:14:18.333195] Namespace(accum_iter=1,
adapter_layer=30,
adapter_len=10,
batch_size=4,
blr=0.009,
data_path='/instruction_dataset/',
device='cuda',
dist_backend='nccl',
dist_on_itp=False,
dist_url='env://',
distributed=True,
epochs=10,
gpu=0,
llama_model_path='/data1/data/sooh-data/llama/',
local_rank=8,
log_dir='./output_dir',
lr=None,
max_seq_len=512,
min_lr=0.0,
model='Llama7B_adapter',
num_workers=10,
output_dir='/data1/data/sooh-data/llama/hipe/checkpoint-prompt1/',
pin_mem=True,
rank=0,
resume='',
seed=0,
start_epoch=0,
warmup_epochs=2,
weight_decay=0.02,
world_size=4)
[09:14:18.542821] [09:14:18.543149] [09:14:18.543294] [09:14:18.543421] [09:14:18.543562]
Traceback (most recent call last):
  File "finetuning_hipe_prompt1.py", line 406, in <module>
    main(args)
  File "finetuning_hipe_prompt1.py", line 279, in main
    dataset_train = InstructionDataset(model_path = args.llama_model_path, max_words=args.max_seq_len, partition='train')
  File "finetuning_hipe_prompt1.py", line 107, in __init__
    entity = inv_entity[template_list.index(label)]
ValueError: ' isthme de Suez' is not in list