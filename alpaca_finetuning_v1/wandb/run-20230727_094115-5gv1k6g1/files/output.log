
| distributed init (rank 0): env://, gpu 0
[09:41:18.332417] job dir: /home/sooh/historical-adapters/alpaca_finetuning_v1
[09:41:18.333052] Namespace(accum_iter=1,
adapter_layer=30,
adapter_len=10,
batch_size=4,
blr=0.009,
data_path='/instruction_dataset/',
device='cuda',
dist_backend='nccl',
dist_on_itp=False,
dist_url='env://',
distributed=True,
epochs=10,
gpu=0,
llama_model_path='/data1/data/sooh-data/llama/',
local_rank=8,
log_dir='./output_dir',
lr=None,
max_seq_len=512,
min_lr=0.0,
model='Llama7B_adapter',
num_workers=10,
output_dir='/data1/data/sooh-data/llama/hipe/checkpoint-prompt1/',
pin_mem=True,
rank=0,
resume='',
seed=0,
start_epoch=0,
warmup_epochs=2,
weight_decay=0.02,
world_size=4)
[09:41:18.790223] [09:41:18.790545] [09:41:18.790664] [09:41:18.790771] [09:41:18.790868]
Traceback (most recent call last):
  File "finetuning_hipe_prompt1.py", line 410, in <module>
    main(args)
  File "finetuning_hipe_prompt1.py", line 284, in main
    dataset_val = InstructionDataset(model_path = args.llama_model_path, max_words=args.max_seq_len, partition='val')
  File "finetuning_hipe_prompt1.py", line 138, in __init__
    total_ans.append(temp_dict)
UnboundLocalError: local variable 'total_ans' referenced before assignment